estimation_buffer: 0.25
estimation_growth_factor: 0.01  # buffer increases by 1% per month
storage_buffer: 0.33  # keep max storage at 75% of disk
storage_display_unit: TB

summary_dates:
  - '2024-08'

vm_os_storage_gb: 50
vm_os_storage_group: 'VM_os'

usage:
  users:
    model: 'date_range_value'
    ranges:
      - ['20240701', '20240801', 1000]

  #Number of forms per user per month
  forms_monthly:
    model: 'derived_factor'
    dependant_field: 'users'
    factor: 300 # Forms per user per month (16000000 forms / 1132 users)

  # Total Forms ever created
  forms_total:
    model: 'cumulative'
    dependant_field: 'forms_monthly'
    start_with: 0   # from ES form index (total docs)


  forms_per_sec_max:
    model: 'derived_factor'
    dependant_field: 'forms_monthly'
    factor: 0.00000666666  # 2 forms per second for 1.1M forms per month

  #Number of cases per user per month
  cases_total:
    model: 'baseline_with_growth'
    dependant_field: 'users'
    baseline: 10
    monthly_growth: 150
    start_with: 0

  # Number of cases updated per user per month
  case_transactions:
    model: 'derived_factor'
    dependant_field: 'forms_monthly'
    factor: 3  # 3 cases per form

  # Total Case Transaction
  case_transactions_total:
    model: 'cumulative'
    dependant_field: 'case_transactions'
    start_with: 0   # 10 per case?

  case_indices:
    model: 'derived_factor'
    dependant_field: 'cases_total'
    factor: 1  # guess

  synclogs_monthly:
    model: 'derived_factor'
    dependant_field: 'users'
    factor: 30

  synclogs_total:
    model: 'cumulative_limited_lifespan'
    dependant_field: 'synclogs_monthly'
    lifespan: 2  # we only keep synclogs for 2 months

  restores_per_second:
    model: 'derived_factor'
    dependant_field: 'synclogs_monthly'
    factor: 0.000001325  # 0.03 restores per second for 22650 restores per month

  requests_per_second:
    model: 'derived_sum'
    dependant_fields:
      - 'restores_per_second'
      - 'forms_per_sec_max'

  kafka_changes:
    model: 'derived_sum'
    dependant_fields:
      - 'forms_monthly'
      - 'cases_total_monthly'
      - 'case_transactions'
      - 'synclogs_monthly'
  kafka_changes_total:
    model: 'cumulative_limited_lifespan'
    dependant_field: 'kafka_changes'
    lifespan: 2  # keep kafka changes for 2 months

services:
  pg_shards:
    static_number: 1
    include_ha_resources: True
    storage:
      group: 'SSD'
      data_models:
        - referenced_field: 'forms_total'
          unit_size: 1200
        - referenced_field: 'cases_total'
          unit_size: 1800
        - referenced_field: 'case_indices'
          unit_size: 380
        - referenced_field: 'case_transactions_total'
          unit_size: 515
    process:
      cores_per_node: 2
      ram_per_node: 8

  pg_main:
    storage_scales_with_nodes: True
    static_number: 1
    include_ha_resources: True
    storage:
      group: 'SSD'
      static_baseline: 100GB
    process:
      cores_per_node: 2
      ram_per_node: 8

  pg_synclogs:
    static_number: 1
    storage:
      group: 'SSD'
      data_models:
        - referenced_field: 'synclogs_total'
          unit_size: 210000
    process:
      cores_per_node: 2
      ram_per_node: 4

  couchdb:
    static_number: 2
    storage:
      group: 'SSD'
      redundancy_factor: 2
      static_baseline: 50GB  # to account for other databases
      override_storage_buffer: 0.8  # space for compaction
      data_models:
        - referenced_field: 'users'
          unit_size: 600000   # disk size / doc count of icds @ 2017-12-13
    process:
      cores_per_node: 2
      ram_per_node: 4

  es_datanode:
    static_number: 1
    storage:
      group: 'SAS'
      data_models:
        - referenced_field: 'forms_total'
          unit_size: 5500
        - referenced_field: 'cases_total'
          unit_size: 1800
    process:
      cores_per_node: 2
      ram_per_node: 16

  riakcs:
    usage_capacity_per_node: 50000
    # avg attachment size of 12560 bytes (11000*0.96 + 50000*0.04)
    # RAM requirement per key = 130b
    # num keys = 10TB / (12560b x 3<redundancy factor>)
    # RAM needed = 130b x num keys = 35GB (64GB avail per node)
    max_storage_per_node: 25TB
#    min_nodes: 3
    storage:
      group: 'SSD'
      redundancy_factor: 3
      static_baseline: 250GB  # to account for exports etc
      data_models:
        - referenced_field: 'forms_total'  # 96% of objects
          unit_size: 15700
    process:
      cores_per_node: 4
      # need to be able to fit all keys in RAM since we're using bitcask backend
      ram_per_node: 16
      ram_model:
      - referenced_field: 'forms_total'
        # key size (45 + 6 + 79) (overhead + bucket + key)
        # bucket = 'blobdb'
        # new keys are smaller but stick with old key length for safety:
        #   new: form/xxxxxxxxxxxxxxuuidxxxxxxxxxxxxxx/Xpi-XM9CZvQ
        #   old: form/xxxxxxxxxxxxxxuuidxxxxxxxxxxxxxx/form.xml.xxxxxxxxxxxxxxuuidxxxxxxxxxxxxxx
        unit_size: 130
      ram_redundancy_factor: 3
      ram_static_baseline: 1  # per node

  pg_ucr:
    static_number: 1
    include_ha_resources: True
    storage:
      # This is a rough estimate.
      # The person case UCR is 35% of total UCR usage.
      group: 'SSD'
      data_models:
        - referenced_field: 'cases_total'  # cumulative
          unit_size: 8000  # estimate based off UCR data Jan 2019
    process:
      cores_per_node: 2
      ram_per_node: 8

  pg_warehouse:
    static_number: 1
    storage:
      group: 'SSD'
      data_models:
        - referenced_field: 'forms_total'
          unit_size: 4000
    process:
      cores_per_node: 2
      ram_per_node: 4

  pillowtop:
    static_number: 1
    storage:
      group: 'VM_other'
      static_baseline: 100GB
      override_storage_buffer: 0
      override_estimation_buffer: 0
    process:
      cores_per_node: 8
      ram_per_node: 32

  celery:
    static_number: 1
    process:
      cores_per_node: 8
      ram_per_node: 32
    storage:
      group: 'VM_other'
      static_baseline: 100GB
      override_storage_buffer: 0
      override_estimation_buffer: 0

  django:
    # 1 gunicorn worker can handle 1 request every 2 seconds
    usage_field: 'requests_per_second'
    storage_scales_with_nodes: True
    process:
      cores_per_node: 4
      ram_per_node: 8
      cores_per_sub_process: 1
      ram_per_sub_process: 0.7
      sub_processes:
        - name: 'gunicorn_worker'
          capacity: 0.5
    storage:
      group: 'VM_other'
      static_baseline: 50GB
      override_storage_buffer: 0
      override_estimation_buffer: 0

  redis:
    static_number: 1
    process:
      cores_per_node: 1
      ram_per_node: 8
      ram_model:
        - referenced_field: 'users'
          unit_size: 50KB
      ram_static_baseline: 4
    storage:
      group: 'SAS'
      data_models:
        - referenced_field: 'users'
          unit_size: 50KB

  nginx:
    static_number: 1
    process:
      cores_per_node: 2
      ram_per_node: 4
    storage:
      group: 'VM_other'
      static_baseline: 50GB  # logs etc
      override_storage_buffer: 0
      override_estimation_buffer: 0

  rabbitmq:  # limits for rabbitmq not clear
    static_number: 1
    storage_scales_with_nodes: True
    process:
      cores_per_node: 2
      ram_per_node: 4
    storage:  # don't have a model for rabbitmq storage
      group: 'SAS'
      static_baseline: 50GB
      override_storage_buffer: 0
      override_estimation_buffer: 0

  kafka:
    static_number: 1
    storage:
      group: 'SAS'
      data_models:
        - referenced_field: 'kafka_changes_total'
          unit_size: 500
    process:
      cores_per_node: 2
      ram_per_node: 4

  formplayer:
    static_number: 1
    process:
      cores_per_node: 2
      ram_per_node: 4
    storage:
      group: 'SAS'
      static_baseline: 100GB
      override_storage_buffer: 0
      override_estimation_buffer: 0
